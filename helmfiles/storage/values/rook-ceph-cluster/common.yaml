operatorNamespace: rook-ceph

pspEnable: false

monitoring:
  enabled: true
  namespace: monitoring

cephClusterSpec:
  mon:
    volumeClaimTemplate:
      spec:
        storageClassName: local-path
        resources:
          requests:
            storage: 10Gi
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - nuc1
              - nuc2
              - nuc3
    mon:
      podAntiAffinity:                                 
        requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:                               
              matchLabels:                               
                app: rook-ceph-mon
  dashboard:
    enabled: false
  storage:
    onlyApplyOSDPlacement: true
    storageClassDeviceSets:
      - name: nuc1
        count: 1
        portable: false
        encrypted: false
        placement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - nuc1
        volumeClaimTemplates:
          - metadata:
              name: rook-ceph-nuc1
            spec:
              resources:
                requests:
                  storage: 800Gi
              storageClassName: manual-block-storage
              volumeMode: Block
              volumeName: rook-ceph-nuc1
              accessModes:
                - ReadWriteOnce
      - name: nuc2
        count: 1
        portable: false
        encrypted: false
        placement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - nuc2
        volumeClaimTemplates:
          - metadata:
              name: rook-ceph-nuc2
            spec:
              resources:
                requests:
                  storage: 800Gi
              storageClassName: manual-block-storage
              volumeMode: Block
              volumeName: rook-ceph-nuc2
              accessModes:
                - ReadWriteOnce
      - name: nuc3
        count: 1
        portable: false
        encrypted: false
        placement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - nuc3
        volumeClaimTemplates:
          - metadata:
              name: rook-ceph-nuc3
            spec:
              resources:
                requests:
                  storage: 800Gi
              storageClassName: manual-block-storage
              volumeMode: Block
              volumeName: rook-ceph-nuc3
              accessModes:
                - ReadWriteOnce
  resources:
    mgr:
      limits:
        cpu: "1000m"
        memory: "1Gi"
      requests:
        cpu: "200m"
        memory: "128Mi"
    mon:
      limits:
        cpu: "1000m"
        memory: "1Gi"
      requests:
        cpu: "200m"
        memory: "128Mi"
    osd:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "200m"
        memory: "128Mi"
    prepareosd:
      # limits: It is not recommended to set limits on the OSD prepare job since it's a one-time burst for memory
      # that must be allowed to complete without an OOM kill
      requests:
        cpu: "200m"
        memory: "50Mi"
    mgr-sidecar:
      limits:
        cpu: "200m"
        memory: "100Mi"
      requests:
        cpu: "100m"
        memory: "40Mi"
    crashcollector:
      limits:
        cpu: "200m"
        memory: "60Mi"
      requests:
        cpu: "100m"
        memory: "60Mi"
    logcollector:
      limits:
        cpu: "200m"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "100Mi"
    cleanup:
      limits:
        cpu: "500m"
        memory: "1Gi"
      requests:
        cpu: "200m"
        memory: "100Mi"

cephFileSystems:
  - name: ceph-fs
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - failureDomain: host
          replicated:
            size: 3
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            cpu: "500m"
            memory: "1Gi"
          requests:
            cpu: "100m"
            memory: "256Mi"
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: true
      name: ceph
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

cephBlockPools: []

cephObjectStores:
  - name: ceph-objectstore
    # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
      dataPool:
        failureDomain: host
        erasureCoded:
          dataChunks: 2
          codingChunks: 1
      preservePoolsOnDelete: true
      gateway:
        port: 80
        resources:
          limits:
            cpu: "500m"
            memory: "1Gi"
          requests:
            cpu: "100m"
            memory: "256Mi"
        instances: 1
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: false