operatorNamespace: rook-ceph

monitoring:
  enabled: true

cephClusterSpec:
  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: local-path
        resources:
          requests:
            storage: 10Gi
  mgr:
    count: 1
    modules:
      - name: pg_autoscaler
        enabled: true
  crashCollector:
    disable: false
  dashboard:
    enabled: false
  storage:
    onlyApplyOSDPlacement: false
    storageClassDeviceSets:
      - name: data
        count: 3
        portable: false
        tuneDeviceClass: true
        tuneFastDeviceClass: false
        encrypted: false
        placement:
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: ScheduleAnyway
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd
                      - rook-ceph-osd-prepare
        preparePlacement:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd-prepare
                  topologyKey: kubernetes.io/hostname
        volumeClaimTemplates:
          - metadata:
              name: data
              # if you are looking at giving your OSD a different CRUSH device class than the one detected by Ceph
              # annotations:
              #   crushDeviceClass: hybrid
            spec:
              resources:
                requests:
                  storage: 10Gi
              # IMPORTANT: Change the storage class depending on your environment
              storageClassName: block-storage
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0

  #     - name: nuc1
  #       count: 1
  #       portable: false
  #       encrypted: false
  #       placement:
  #         nodeAffinity:
  #           requiredDuringSchedulingIgnoredDuringExecution:
  #             nodeSelectorTerms:
  #             - matchExpressions:
  #               - key: kubernetes.io/hostname
  #                 operator: In
  #                 values:
  #                 - nuc1
  #       volumeClaimTemplates:
  #         - metadata:
  #             name: rook-ceph-nuc1
  #           spec:
  #             resources:
  #               requests:
  #                 storage: 800Gi
  #             storageClassName: block-storage
  #             volumeMode: Block
  #             volumeName: rook-ceph-nuc1
  #             accessModes:
  #               - ReadWriteOnce
  #     - name: nuc2
  #       count: 1
  #       portable: false
  #       encrypted: false
  #       placement:
  #         nodeAffinity:
  #           requiredDuringSchedulingIgnoredDuringExecution:
  #             nodeSelectorTerms:
  #             - matchExpressions:
  #               - key: kubernetes.io/hostname
  #                 operator: In
  #                 values:
  #                 - nuc2
  #       volumeClaimTemplates:
  #         - metadata:
  #             name: rook-ceph-nuc2
  #           spec:
  #             resources:
  #               requests:
  #                 storage: 800Gi
  #             storageClassName: block-storage
  #             volumeMode: Block
  #             volumeName: rook-ceph-nuc2
  #             accessModes:
  #               - ReadWriteOnce
  #     - name: nuc3
  #       count: 1
  #       portable: false
  #       encrypted: false
  #       placement:
  #         nodeAffinity:
  #           requiredDuringSchedulingIgnoredDuringExecution:
  #             nodeSelectorTerms:
  #             - matchExpressions:
  #               - key: kubernetes.io/hostname
  #                 operator: In
  #                 values:
  #                 - nuc3
  #       volumeClaimTemplates:
  #         - metadata:
  #             name: rook-ceph-nuc3
  #           spec:
  #             resources:
  #               requests:
  #                 storage: 800Gi
  #             storageClassName: block-storage
  #             volumeMode: Block
  #             volumeName: rook-ceph-nuc3
  #             accessModes:
  #               - ReadWriteOnce
  resources:
    mgr:
      limits:
        cpu: "1000m"
        memory: "1Gi"
      requests:
        cpu: "10m"
        memory: "128Mi"
    mon:
      limits:
        cpu: "1000m"
        memory: "1Gi"
      requests:
        cpu: "50m"
        memory: "128Mi"
    osd:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "10m"
        memory: "128Mi"
    prepareosd:
      # limits: It is not recommended to set limits on the OSD prepare job since it's a one-time burst for memory
      # that must be allowed to complete without an OOM kill
      requests:
        cpu: "200m"
        memory: "50Mi"
    mgr-sidecar:
      limits:
        cpu: "200m"
        memory: "100Mi"
      requests:
        cpu: "100m"
        memory: "40Mi"
    crashcollector:
      limits:
        cpu: "20m"
        memory: "60Mi"
      requests:
        cpu: "5m"
        memory: "60Mi"
    logcollector:
      limits:
        cpu: "20m"
        memory: "60Mi"
      requests:
        cpu: "5m"
        memory: "60Mi"
    cleanup:
      limits:
        cpu: "500m"
        memory: "1Gi"
      requests:
        cpu: "200m"
        memory: "100Mi"

cephFileSystems:
  - name: ceph-fs
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - name: replicated
          failureDomain: host
          replicated:
            size: 3
            requireSafeReplicaSize: true
          parameters:
            compression_mode:
              none
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            cpu: "500m"
            memory: "1Gi"
          requests:
            cpu: "100m"
            memory: "256Mi"
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: true
      name: ceph
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      fsName: ceph-fs
      pool: replicated
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

# cephBlockPools: []

# cephObjectStores:
#   - name: ceph-objectstore
#     # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
#     spec:
#       metadataPool:
#         failureDomain: host
#         replicated:
#           size: 3
#       dataPool:
#         failureDomain: host
#         erasureCoded:
#           dataChunks: 2
#           codingChunks: 1
#       preservePoolsOnDelete: true
#       gateway:
#         port: 80
#         resources:
#           limits:
#             cpu: "500m"
#             memory: "1Gi"
#           requests:
#             cpu: "100m"
#             memory: "256Mi"
#         instances: 1
#         priorityClassName: system-cluster-critical
#     storageClass:
#       enabled: false
